# Q1 â€“ Tokenization & Fill-in-the-Blank (NLP with Transformers)

This project demonstrates how different tokenization algorithms and a masked language model can be used to:
- Compare tokenization behavior across **BPE**, **WordPiece**, and **SentencePiece**.
- Mask tokens in a sentence and predict them using a fill-mask pipeline.

---

## ğŸ”§ Dependencies

Install the required Python packages:

```bash
pip install transformers sentencepiece
```

### ğŸš€ How to Run
Execute the script from within the q1 folder:

```python
python tokenise.py
```
This will:
Create a file compare.md containing tokenized outputs and analysis (Part 1).

Create a file predictions.json with top-3 fill-mask predictions for two masked tokens (Part 2).

### ğŸ“ File Structure
```bash
q1/
â”œâ”€â”€ tokenise.py         # Main Python script for both parts
â”œâ”€â”€ compare.md          # Output of tokenization comparison
â”œâ”€â”€ predictions.json    # Output of masked token predictions
â””â”€â”€ Readme.md           # Instructions and overview
```

### ğŸ“Œ Notes
The masked language model used for predictions is roberta-base because models like mistralai/Mistral-7B-Instruct do not support fill-mask tasks (they are causal, not masked).

The tokenizer-model pairings used:

- GPT-2 â†’ BPE

- BERT â†’ WordPiece

- T5 â†’ SentencePiece (Unigram)

### âœï¸ Sample Sentence Used
```text
The cat sat on the mat because it was tired.

Masked version for Part 2:
```
```text
The <mask> sat on the mat because it was <mask>.
```

